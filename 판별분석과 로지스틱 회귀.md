## 판별 분석

- 초장기의 통계 분류 방법
- 보통 예측변수가 정규분포를 따르는 연속적인 변수라는 가정이 있지만 실제로는 정규분포에서 벗어나거나 이진 예측변수에 대해서도 잘 동작

#### 피셔의 선형판별

- 그룹 안의 편차와 다른 그룹 간의 편차를 구분
- LDA(선형판별분석)은 '내부' 제곱합 **SS<sub>내부</sub>(그룹 내부의 변동)에 대한 '사이' 제곱합 SS<sub>사이</sub> (두 그룹 사이의 편차)의 비율을 최대화**하는 것을 목표로 함
  - 내부 제곱합 : 공분산행렬에 의해 가중치가 적용된, 각 그룹 내의 평균을 주변으로 퍼져 있는 정도
  - 사이 제곱합 : 두 그룹 평균 사이의 거리 제곱
  - **사이 제곱합은 최대화, 내부 제곱합은 최소화하는 것이 두 그룹을 가장 명확하게 나누는 방법**

```R
library(MASS)

setwd("C:/Users/0864h/Desktop/practical-statistics-for-data-scientists-master/data")
loan3000 <- read.csv("loan3000.csv")

loan_lda <- lda(outcome ~ borrower_score + payment_inc_ratio, data=loan3000)

# 선형판별자 가중치
loan_lda$scaling
#                           LD1
# borrower_score     7.17583880
# payment_inc_ratio -0.09967559

pred <- predict(loan_lda)
head(pred$posterior)
# 연체, 상환에 대한 확률값을 반환
#     default  paid off
# 1 0.5535437 0.4464563
# 2 0.5589534 0.4410466
# 3 0.2726962 0.7273038
# 4 0.5062538 0.4937462
# 5 0.6099525 0.3900475
# 6 0.4107406 0.5892594
```

```python
import pandas as pd
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

loan3000 = pd.read_csv("C:/Users/0864h/Desktop/practical-statistics-for-data-scientists-master/data/loan3000.csv")

predictors = ["borrower_score", "payment_inc_ratio"]
outcome = "outcome"

X = loan3000[predictors]
y = loan3000[outcome]

LDA = LinearDiscriminantAnalysis()
LDA.fit(X, y)

res = pd.DataFrame(LDA.scalings_, index=predictors)

print(res)
#                           0
# borrower_score     7.175839
# payment_inc_ratio -0.099676

pred = pd.DataFrame(LDA.predict_proba(X), columns=LDA.classes_) 
print(pred.head())
#     default  paid off
# 0  0.553544  0.446456
# 1  0.558953  0.441047
# 2  0.272696  0.727304
# 3  0.506254  0.493746
# 4  0.609952  0.390048

# 스케일링 계수와 평균의 중간값을 이용해 결정 경계를 구함
center = np.mean(LDA.means_, axis=0)
slope = LDA.scalings_[0] / LDA.scalings_[1]
print(slope)
# -71.99193766
intercept = center[1] - center[0] * slope
print(intercept)
# 44.226798

# payment_inc_ratio가 0 또는 20이 되는 borrower_scroe 값을 구함
x_0 = (0 - intercept) / slope
x_20 = (20 - intercept) / slope
print(x_0, x_20)

lda_df = pd.concat([loan3000, pred["default"]], axis=1)

fig, ax = plt.subplots(figsize=(4, 4))
g = sns.scatterplot(x="borrower_score", y="payment_inc_ratio", hue="default", data=lda_df, palette=sns.diverging_palette(240, 10, n=9, as_cmap=True),
                    ax=ax)

ax.set_ylim(0, 20)
ax.set_xlim(0.15, 0.8)
ax.plot((x_20, x_0), (0, 20), color="k", linewidth=3)

plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/146677331-764235ff-edcc-45c5-8262-1112682abdf8.png" width=50% />

- 실선으로부터 양방향으로 멀리 떨어진 예측 결과일수록 신뢰도가 높음



## 로지스틱 회귀

- 결과변수를 이진값으로 생각하기보다 라벨이 '1'이 될 확률을 p로 생각하면 로지스틱 회귀는 **로지스틱 반응 or 역 로짓 함수**를 적용해서 p를 모델링함 **(이 변환을 통해 p가 항상 0에서 1사이에 오도록 할 수 있음)**
- 오즈 (odds) : '실패'(0)에 대한 '성공'(1)의 비율
- 오즈비 : '성공'(1)과 '실패'(0)의 비율 **(사건이 발생할 확률을 사건이 발생하지 않을 확률로 나눈 비율)**

  - 오즈(Y = 1) = p / 1 - p
  - 0부터 양의 무한대까지 값을 가질 수 있음
- p = 오즈 / 1 + 오즈
  - 오즈(Y = 1) = exp(beta0 + beta1 * x1 + beta2 * x2 + ... )
  - 오즈비 = 오즈 (Y = 1 | X = 1) / 오즈 (Y = 1 | X = 0)
    - 오즈비가 2이면 X = 1 일 때, Y = 1의 오즈가 X = 0 일 때의 보다 두 배 더 높음을 의미
- 로짓 (logit) : ±∞의 범위에서 어떤 클래스에 속할 확률을 결정하는 함수
  - 선형모형과 비슷한 형태의 모델을 만들기 위함
  - log(오즈(Y = 1)) = beta0 + beta1 * x1 + beta2 * x2 + ... 
  - 로그 오즈 (로짓) 함수는 0과 1 사이의 확률 p를 ±∞의 값으로 매핑
  - 로지스틱 회귀분석에서 계수에 해당 

- 로지스틱 함수는 로짓 함수의 역함수
  - ±∞ 값을 가지는 입력변수를 0 ~ 1 사이의 값을 가지는 출력변수로 변환
  - 로지스틱 회귀의 예측값은 p hat = 1 / 1 + exp(-Y hat)으로 로짓 Y hat에 관한 값(확률)임 

